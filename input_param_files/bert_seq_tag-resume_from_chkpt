Vineet Kumar, sioom.ai

This input file has user-settable hyper-parameters to resume training of
   a checkpointed model. Testing is optional
Use Case: A user stops training for any reason and then later resumes it. Training
   can be stopped through the keystroke ctrl-c or by using appropriate
   hyperparameters.

Note the following:
	(1) This file name should be last in the command-line.
	(2) Do NOT change the order of python-dictionaries in this file.
	(3) The default directory is "sequence-tagging"
	(4) All the dictionaries MUST be present even if they are empty
	(5) Unless specified, if a dictionary is empty, its contents will be replaced by the 
	    corresponding dictionary from the checkpoint file that is loaded
	(6) Some dicts -- such as misc -- must not be empty

Command-line:
-------------
python3 Main.py input_param_files/bert_seq_tag-resume_from_chkpt 


parameters for python-dictionary 'misc'. ***Must specify the contents of this dict***
-
{'save_top_k': 4, 'train': True}


parameters for python-dictionary 'optz_sched'. ***Contents of this dict are replaced by corresponding checkpoint dict*** 
- 
{}


parameters for python-dictionary 'data'
***Either keep this dict empty or specifiy All key-values
- 
#{'dataframes_dirPath': 'experiments/2', 'batch_sizes': {'train': 2, 'val': 2, 'test': 32}} 
{}


parameters for python-dictionary 'trainer'
- 
{'accelerator': 'gpu', 'devices': 1, 'max_epochs': 10000, 'log_every_n_steps': 100, 'accumulate_grad_batches': 32, 'gradient_clip_val': 0.5}


parameters for python-dictionary 'model_init'. ***Contents of this dict are replaced by corresponding checkpoint dict*** 
- 
{}


parameters for python-dictionary 'ld_resume_chkpt'. ***Must specify the contents of this dict***
- 
{'resume_from_checkpoint': '/home/vin/sequence-tagging/experiments/2/model=bert,model_type=bert-large-uncased,tokenizer_type=bert/ckpts_v0/checkpoints/optz=Adam,lr=1e-05,lr_sched=ReduceLROnPlateau,mode=min,patience=4,factor=0.5,epoch=04-val_loss=0.01194.ckpt'}
