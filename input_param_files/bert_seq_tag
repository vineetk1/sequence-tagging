Vineet Kumar, sioom.ai

This input file has user-settable hyper-parameters for training and/or testing
   a model.

Note the following:
	(1) This file name should be last in the command-line.
	(2) Do NOT change the order of python-dictionaries in this file.
	(3) The default directory is "sequence-tagging"
	(4) All the dictionaries MUST be present even if they are empty
 
Command-line:
-------------
python3 Main.py input_param_files/bert_seq_tag 
python3 -m pdb Main.py input_param_files/bert_seq_tag 
*************** NOTE: make sure that dataframes_dirPath is correct ***********

parameters for python-dictionary 'misc'
-
{'save_top_k': 2, 'train': True}


parameters for python-dictionary 'optz_sched'
- 
{'optz': 'Adam', 'optz_params': {'lr': 1e-5}, 'lr_sched': 'ReduceLROnPlateau', 'lr_sched_params': {'mode': 'min', 'patience': 4, 'factor': 5e-1}} 

parameters for python-dictionary 'data'
- 
{'dataframes_dirPath': 'experiments/14', 'batch_sizes': {'train': 1, 'val': 32, 'test': 64, 'predict': 64}} 


parameters for python-dictionary 'trainer'
- 
{'accelerator': 'gpu', 'devices': 1, 'max_epochs': 10000, 'log_every_n_steps': 100, 'accumulate_grad_batches': 32, 'gradient_clip_val': 0.5}
#{'accelerator': 'gpu', 'devices': 1, 'max_epochs': 5, 'log_every_n_steps': 100, 'accumulate_grad_batches': 32, 'stochastic_weight_avg': True, 'gradient_clip_val': 0.5}
#{'accelerator': 'gpu', 'devices': 1, 'max_epochs': 1, 'auto_lr_find': False, 'auto_scale_batch_size': True}
#{'accelerator': 'gpu', 'devices': 1, 'max_epochs': 1, 'auto_lr_find': True, 'auto_scale_batch_size': False}


parameters for python-dictionary 'model_init'
Dropouts are used ONLY during Training; dropouts are disabled by Lightning during validation or testing or Predicting
- 
#{'model': 'bert', 'model_type': 'bert-large-uncased', 'tokenizer_type': 'bert'}
{'model': 'bert', 'model_type': 'bert-large-uncased', 'tokenizer_type': 'bert', 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'classification_head_dropout': 0.1, 'loss_func_class_weights': False}
#{'model': 'bert', 'model_type': 'bert-base-uncased', 'tokenizer_type': 'bert', 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'classification_head_dropout': 0.1, 'loss_func_class_weights': False}
#{'model': 'bert', 'model_type': 'bert-large-uncased', 'tokenizer_type': 'bert', 'hidden_dropout_prob': 0.1, 'attention_probs_dropout_prob': 0.1, 'classification_head_dropout': 0.1, 'loss_func_class_weights': True}
#{'model': 'bert', 'model_type': 'bert-large-uncased', 'tokenizer_type': 'bert', 'hidden_dropout_prob': 0.2, 'attention_probs_dropout_prob': 0.2, 'classification_head_dropout': 0.2, 'loss_func_class_weights': True}
#{'model': 'bert', 'model_type': 'bert-large-uncased', 'tokenizer_type': 'bert', 'classification_head_dropout': 0.3, 'tokenizer_init': {'tokenizer.model_max_length': 20, 'tokenizer.truncation_side':'left'}}


parameters for python-dictionary 'ld_resume_chkpt'. This dictionary MUST be empty
- 
{}
